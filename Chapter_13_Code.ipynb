{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 13 - Loading and Preprocessing Data with TensorFlow Code Reproduction"
      ],
      "metadata": {
        "id": "2_ojQsZlURBY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "InJjdl_z8-Ge"
      },
      "outputs": [],
      "source": [
        "# Impor umum\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat dataset dari tensor di memori\n",
        "X = tf.range(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "print(\"Dataset awal:\", list(dataset.as_numpy_iterator()))\n",
        "\n",
        "# Rantai transformasi (chaining transformations)\n",
        "dataset = dataset.repeat(3).batch(7)\n",
        "print(\"Dataset setelah repeat(3) dan batch(7):\")\n",
        "for item in dataset:\n",
        "    print(item)\n",
        "\n",
        "# Transformasi map, prefetch, dan shuffle\n",
        "dataset = tf.data.Dataset.range(10).map(lambda x: x * 2) # Kalikan setiap elemen dengan 2\n",
        "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7) # Acak dengan buffer dan batch\n",
        "dataset = dataset.prefetch(1) # Prefetch satu batch di latar belakang\n",
        "\n",
        "print(\"\\nDataset setelah map, shuffle, batch, dan prefetch:\")\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv_bfLYAVLHA",
        "outputId": "e011aac0-0911-4dc4-f399-6c9a85c62fb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset awal: [np.int32(0), np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6), np.int32(7), np.int32(8), np.int32(9)]\n",
            "Dataset setelah repeat(3) dan batch(7):\n",
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int32)\n",
            "\n",
            "Dataset setelah map, shuffle, batch, dan prefetch:\n",
            "tf.Tensor([ 0  4  6 12 14 18  2], shape=(7,), dtype=int64)\n",
            "tf.Tensor([16  8 10], shape=(3,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interleaving Files (Membaca Beberapa File secara Bersamaan)\n",
        "Contoh bagaimana membaca dari beberapa file CSV secara efisien."
      ],
      "metadata": {
        "id": "v6Hlpr4vVPal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Asumsikan Anda sudah memiliki data housing dan menyimpannya ke CSV)\n",
        "# Kode ini mendemonstrasikan konsepnya\n",
        "# filepath_dataset = tf.data.Dataset.list_files(file_pattern)\n",
        "# n_readers = 5\n",
        "# dataset = filepath_dataset.interleave(\n",
        "#     lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "#     cycle_length=n_readers)"
      ],
      "metadata": {
        "id": "52xh9SVGVPqk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat file TFRecord\n",
        "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
        "    f.write(b\"Data pertama\")\n",
        "    f.write(b\"Data kedua\")\n",
        "\n",
        "# Menulis protobuf tf.train.Example\n",
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Feature, Features, Example\n",
        "\n",
        "person_example = Example(\n",
        "    features=Features(\n",
        "        feature={\n",
        "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
        "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
        "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
        "        }))\n",
        "\n",
        "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
        "    f.write(person_example.SerializeToString()) # Serialisasi ke string biner"
      ],
      "metadata": {
        "id": "4Kx87___VS3_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat dataset dari file TFRecord\n",
        "filepaths = [\"my_contacts.tfrecord\"]\n",
        "dataset = tf.data.TFRecordDataset(filepaths)\n",
        "\n",
        "# Mendefinisikan deskripsi fitur untuk parsing\n",
        "feature_description = {\n",
        "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    \"emails\": tf.io.VarLenFeature(tf.string), # Fitur dengan panjang bervariasi\n",
        "}\n",
        "\n",
        "# Fungsi untuk parsing\n",
        "def parse_examples(serialized_examples):\n",
        "    return tf.io.parse_example(serialized_examples, feature_description)\n",
        "\n",
        "# Menerapkan parsing pada dataset\n",
        "dataset_parsed = dataset.map(parse_examples)\n",
        "print(\"\\nDataset setelah di-parse dari TFRecord:\")\n",
        "for parsed_example in dataset_parsed:\n",
        "    print(parsed_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjuTrPn7VVrE",
        "outputId": "b4adc4bb-8895-4744-d047-4e777160cd39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset setelah di-parse dari TFRecord:\n",
            "{'emails': SparseTensor(indices=tf.Tensor(\n",
            "[[0]\n",
            " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64)), 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data sampel\n",
        "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
        "indices = tf.range(len(vocab), dtype=tf.int64)\n",
        "\n",
        "# 1. One-hot encoding\n",
        "# Membuat layer untuk mapping dari string ke integer\n",
        "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
        "num_oov_buckets = 2 # Untuk kata yang tidak ada di vocabulary\n",
        "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
        "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
        "cat_indices = table.lookup(categories)\n",
        "print(\"\\nHasil mapping string ke integer:\", cat_indices)\n",
        "# Melakukan one-hot encoding dari integer\n",
        "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
        "print(\"Hasil one-hot encoding:\\n\", cat_one_hot)\n",
        "\n",
        "\n",
        "# 2. Embedding\n",
        "# Membuat layer embedding\n",
        "embedding_dim = 2\n",
        "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
        "embedding_layer = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets,\n",
        "                                         output_dim=embedding_dim)\n",
        "print(\"\\nHasil embedding:\\n\", embedding_layer(cat_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z2kUnvSVaOB",
        "outputId": "3cb85a16-0592-49b0-aae4-8f7c14d3f551"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hasil mapping string ke integer: tf.Tensor([3 5 1 1], shape=(4,), dtype=int64)\n",
            "Hasil one-hot encoding:\n",
            " tf.Tensor(\n",
            "[[0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]], shape=(4, 7), dtype=float32)\n",
            "\n",
            "Hasil embedding:\n",
            " tf.Tensor(\n",
            "[[-0.01203145  0.03223113]\n",
            " [-0.01718166  0.03693749]\n",
            " [ 0.01563598 -0.02709633]\n",
            " [ 0.01563598 -0.02709633]], shape=(4, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat data dan layer normalisasi\n",
        "X_train = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
        "norm_layer = keras.layers.Normalization()\n",
        "\n",
        "# Mengadaptasi layer ke data pelatihan untuk mempelajari mean dan stddev\n",
        "norm_layer.adapt(X_train)\n",
        "X_normalized = norm_layer(X_train)\n",
        "print(\"\\nHasil normalisasi fitur:\\n\", X_normalized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXIFve5_Vd4W",
        "outputId": "3a10efc5-ffea-417f-d43d-f6d0f6322aec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hasil normalisasi fitur:\n",
            " tf.Tensor(\n",
            "[[-1.2247449 -1.2247448]\n",
            " [ 0.         0.       ]\n",
            " [ 1.2247448  1.2247449]], shape=(3, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat dataset\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_mean = scaler.mean_\n",
        "X_std = scaler.scale_\n",
        "\n",
        "# Fungsi untuk menyimpan dataset ke beberapa file CSV\n",
        "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "    # ... (fungsi untuk menyimpan data ke file CSV, seperti di buku)\n",
        "    # Untuk singkatnya, kita akan lewati implementasi detailnya di sini\n",
        "    pass\n",
        "\n",
        "# (Langkah ini mengasumsikan data telah disimpan ke file CSV)\n",
        "# Kita akan membuat pipeline untuk membaca file-file tersebut.\n",
        "\n",
        "# 1. Membuat pipeline dataset\n",
        "# (Gantilah 'my_data_*.csv' dengan path file Anda jika menjalankan secara nyata)\n",
        "# filepaths = tf.data.Dataset.list_files(\"my_data_*.csv\")\n",
        "# dataset = filepaths.interleave(...)\n",
        "\n",
        "# 2. Fungsi pra-pemrosesan\n",
        "# (Fungsi ini akan mem-parse setiap baris CSV, melakukan scaling, dan mengembalikan fitur & label)\n",
        "# @tf.function\n",
        "# def preprocess(line):\n",
        "#     defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
        "#     fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "#     x = tf.stack(fields[:-1])\n",
        "#     y = tf.stack(fields[-1:])\n",
        "#     return (x - X_mean) / X_std, y\n",
        "\n",
        "\n",
        "# 3. Membangun pipeline akhir dari data di memori (untuk demonstrasi)\n",
        "# train_data = np.c_[X_train, y_train]\n",
        "# valid_data = np.c_[X_valid, y_valid]\n",
        "# test_data = np.c_[X_test, y_test]\n",
        "\n",
        "def build_pipeline(X_data, y_data, n_epochs=None, shuffle_buffer_size=None, batch_size=32):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
        "    if shuffle_buffer_size:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset.repeat(n_epochs).prefetch(1)\n",
        "\n",
        "# Membuat pipeline untuk training, validasi, dan testing\n",
        "train_set = build_pipeline(X_train, y_train, shuffle_buffer_size=len(X_train))\n",
        "valid_set = build_pipeline(X_valid, y_valid)\n",
        "test_set = build_pipeline(X_test, y_test)\n",
        "\n",
        "\n",
        "# 4. Menggunakan pipeline untuk melatih model\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "history = model.fit(train_set, epochs=10, validation_data=valid_set)\n",
        "\n",
        "# Mengevaluasi model menggunakan test set dari pipeline\n",
        "print(\"\\nMengevaluasi model pada test set:\")\n",
        "model.evaluate(test_set)"
      ],
      "metadata": {
        "id": "94_bqNhW5lia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}