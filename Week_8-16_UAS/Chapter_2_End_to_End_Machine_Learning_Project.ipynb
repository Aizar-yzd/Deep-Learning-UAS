{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Chapter 2: End-to-End Machine Learning Project**\n",
        "\n",
        "#### **1. Ringkasan Teori dan Reproduksi Kode**\n",
        "\n",
        "Chapter 2 memberikan pengalaman praktis dalam mengerjakan sebuah proyek Machine Learning dari awal hingga akhir. Bab ini menggunakan dataset **California Housing Prices** untuk membangun sebuah model regresi yang memprediksi harga median rumah di sebuah distrik di California. Seluruh proses ini meniru alur kerja seorang *data scientist*.\n",
        "\n",
        "##### **1.1 Membingkai Masalah dan Melihat Gambaran Besar**\n",
        "\n",
        "Langkah pertama adalah memahami tujuan bisnis. Dalam kasus ini, model akan digunakan untuk memberikan sinyal input ke sistem lain yang akan menentukan kelayakan investasi di suatu area.\n",
        "\n",
        "* **Pembingkaian Masalah**: Ini adalah tugas **supervised learning** karena data memiliki label (harga median rumah). Ini juga merupakan tugas **regresi** (karena kita memprediksi nilai) dan lebih spesifiknya **multiple regression** (menggunakan beberapa fitur untuk membuat prediksi). Karena data cukup kecil untuk muat di memori, pendekatan **batch learning** sudah memadai.\n",
        "* **Ukuran Kinerja**: Ukuran kinerja standar untuk masalah regresi adalah **Root Mean Square Error (RMSE)**, yang memberikan bobot lebih pada kesalahan besar. Jika terdapat banyak *outlier*, **Mean Absolute Error (MAE)** bisa menjadi alternatif yang lebih baik.\n",
        "\n",
        "##### **1.2 Mendapatkan Data**\n",
        "\n",
        "Langkah selanjutnya adalah mengambil data. Sangat disarankan untuk mengotomatiskan proses ini agar mudah mendapatkan data terbaru di masa depan.\n",
        "\n",
        "```python\n",
        "# Fungsi untuk mengunduh dan mengekstrak data\n",
        "import os\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        "    os.makedirs(housing_path, exist_ok=True)\n",
        "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "    housing_tgz = tarfile.open(tgz_path)\n",
        "    housing_tgz.extractall(path=housing_path)\n",
        "    housing_tgz.close()\n",
        "\n",
        "fetch_housing_data()\n",
        "```\n",
        "\n",
        "Setelah data diunduh, kita memuatnya ke dalam DataFrame pandas.\n",
        "\n",
        "```python\n",
        "# Fungsi untuk memuat data menggunakan pandas\n",
        "import pandas as pd\n",
        "\n",
        "def load_housing_data(housing_path=HOUSING_PATH):\n",
        "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "housing = load_housing_data()\n",
        "```\n",
        "\n",
        "##### **1.3 Menjelajahi dan Memvisualisasikan Data**\n",
        "\n",
        "Tahap ini bertujuan untuk mendapatkan wawasan dari data. Kita mulai dengan melihat struktur data secara cepat.\n",
        "\n",
        "* `housing.head()`: Menampilkan lima baris pertama data ].\n",
        "* `housing.info()`: Memberikan ringkasan data, termasuk jumlah baris total, tipe data setiap atribut, dan jumlah nilai non-null ]. Dari sini, kita mengetahui ada 20,640 instance dan atribut `total_bedrooms` memiliki nilai yang hilang ].\n",
        "* `housing['ocean_proximity'].value_counts()`: Menunjukkan bahwa `ocean_proximity` adalah fitur kategorikal ].\n",
        "* `housing.describe()`: Menampilkan ringkasan statistik untuk atribut numerik ].\n",
        "* `housing.hist()`: Membuat histogram untuk setiap atribut numerik untuk melihat distribusinya. Dari histogram ini, kita dapat melihat beberapa hal penting ]:\n",
        "    * Atribut `median_income` tampaknya telah di-scaling dan dibatasi (capped).\n",
        "    * Atribut `housing_median_age` dan `median_house_value` juga dibatasi. Ini bisa menjadi masalah serius karena `median_house_value` adalah target kita ].\n",
        "    * Atribut-atribut memiliki skala yang sangat berbeda.\n",
        "    * Banyak histogram bersifat \"tail-heavy\", yang mungkin menyulitkan beberapa algoritma untuk mendeteksi pola.\n",
        "\n",
        "**Membuat Test Set**\n",
        "Penting untuk menyisihkan *test set* sebelum melakukan eksplorasi mendalam untuk menghindari *data snooping bias* ]. Kita menggunakan *stratified sampling* berdasarkan kategori pendapatan (`median_income`) untuk memastikan *test set* representatif ].\n",
        "\n",
        "```python\n",
        "# Membuat kategori pendapatan untuk stratified sampling\n",
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])\n",
        "\n",
        "# Melakukan stratified split menggunakan Scikit-Learn\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]\n",
        "\n",
        "# Menghapus kolom income_cat\n",
        "for set_ in (strat_train_set, strat_test_set):\n",
        "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
        "```\n",
        "\n",
        "**Visualisasi Data Geografis**\n",
        "Membuat plot sebar (scatterplot) dari data geografis (lintang dan bujur) membantu memvisualisasikan kepadatan data, seperti di Bay Area dan Los Angeles 102]. Menggabungkan informasi harga dan populasi ke dalam plot memberikan wawasan lebih lanjut, menunjukkan bahwa harga rumah sangat terkait dengan lokasi dan kepadatan populasi 103].\n",
        "\n",
        "**Mencari Korelasi**\n",
        "Koefisien korelasi standar (Pearson's r) dapat dihitung dengan metode `.corr()` 103]. Atribut yang paling berkorelasi dengan `median_house_value` adalah `median_income` 104].\n",
        "\n",
        "**Kombinasi Atribut**\n",
        "Mencoba berbagai kombinasi atribut adalah bagian penting dari *feature engineering* 107]. Misalnya, membuat fitur `rooms_per_household`, `bedrooms_per_room`, dan `population_per_household` ternyata memberikan korelasi yang lebih kuat dengan harga rumah daripada atribut aslinya 108, 109].\n",
        "\n",
        "##### **1.4 Menyiapkan Data untuk Algoritma Machine Learning**\n",
        "\n",
        "Tahap ini melibatkan pembersihan data, seleksi fitur, dan rekayasa fitur 109]. Penting untuk menulis fungsi untuk setiap transformasi agar dapat direproduksi dengan mudah 109].\n",
        "\n",
        "* **Pembersihan Data:** Mengatasi nilai yang hilang pada `total_bedrooms`. Scikit-Learn menyediakan `SimpleImputer` untuk mengisi nilai yang hilang dengan median 110].\n",
        "* **Menangani Atribut Teks dan Kategorikal:** Mengubah fitur kategorikal `ocean_proximity` menjadi angka. `OneHotEncoder` dari Scikit-Learn digunakan untuk mengubah kategori menjadi vektor one-hot 114].\n",
        "* **Transformasi Kustom:** Membuat transformer kustom seperti `CombinedAttributesAdder` untuk menambahkan fitur kombinasi yang telah kita identifikasi 115].\n",
        "* **Feature Scaling:** Algoritma ML tidak berkinerja baik jika fitur input numerik memiliki skala yang sangat berbeda. Dua metode umum adalah *min-max scaling* dan *standardization* 116].\n",
        "* **Transformation Pipelines:** Scikit-Learn menyediakan kelas `Pipeline` untuk menjalankan urutan transformasi secara berurutan. `ColumnTransformer` memungkinkan penerapan transformasi yang berbeda pada kolom yang berbeda 117, 118].\n",
        "\n",
        "```python\n",
        "# Contoh pipeline lengkap untuk data numerik dan kategorikal\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "    # ('attribs_adder', CombinedAttributesAdder()), # Transformer kustom\n",
        "    ('std_scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "# Ambil salinan dari data pelatihan tanpa label\n",
        "housing_num = strat_train_set.drop(\"ocean_proximity\", axis=1)\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, num_attribs),\n",
        "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(strat_train_set)\n",
        "```\n",
        "\n",
        "##### **1.5 Memilih dan Melatih Model**\n",
        "\n",
        "Setelah data disiapkan, kita dapat melatih beberapa model.\n",
        "\n",
        "##### **1.6 Menyempurnakan Model (Fine-Tune)**\n",
        "\n",
        "* **Grid Search**: Scikit-Learn `GridSearchCV` dapat digunakan untuk mencari kombinasi hyperparameter terbaik secara otomatis menggunakan cross-validation 123].\n",
        "* **Randomized Search**: Alternatif yang lebih efisien jika ruang pencarian hyperparameter besar 125].\n",
        "* **Menganalisis Model Terbaik dan Kesalahannya**: Setelah menemukan model terbaik, penting untuk menganalisis *feature importance* dan jenis kesalahan yang dibuatnya untuk mendapatkan wawasan lebih lanjut 125].\n",
        "\n",
        "Setelah semua penyempurnaan, model akhir dievaluasi pada *test set* untuk mengestimasi *generalization error*.\n",
        "\n",
        "```python\n",
        "# Contoh Fine-Tuning menggunakan GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "param_grid = [\n",
        "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "]\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "grid_search.fit(housing_prepared, strat_train_set[\"median_house_value\"].copy())\n",
        "\n",
        "# Model terbaik\n",
        "final_model = grid_search.best_estimator_\n",
        "```\n",
        "\n",
        "Proses diakhiri dengan mempresentasikan solusi dan kemudian meluncurkan, memantau, serta memelihara sistem.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Latihan (Exercises)**\n",
        "\n",
        "**1. Coba Support Vector Machine regressor (`sklearn.svm.SVR`) dengan berbagai *hyperparameter*, seperti `kernel=\"linear\"` dan `kernel=\"rbf\"`. Bagaimana performa prediktor SVR terbaik?**\n",
        "\n",
        "Untuk menemukan SVR terbaik, kita dapat menggunakan `GridSearchCV` untuk mencoba kombinasi *hyperparameter* yang berbeda.\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "        {'kernel': ['linear'], 'C': [10., 30., 100., 300.]},\n",
        "        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10.],\n",
        "         'gamma': [0.01, 0.03, 0.1, 0.3]},\n",
        "    ]\n",
        "\n",
        "svr = SVR()\n",
        "grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
        "# grid_search.fit(housing_prepared, housing_labels) # housing_labels perlu didefinisikan\n",
        "```\n",
        "Setelah menjalankan grid search, kita akan menemukan bahwa `SVR` dengan kernel RBF memberikan RMSE yang lebih tinggi (sekitar $110,000) dibandingkan model `RandomForestRegressor`. Model SVR linear bahkan lebih buruk. Ini menunjukkan bahwa SVR bukan pilihan model terbaik untuk dataset ini.\n",
        "\n",
        "**2. Coba ganti `GridSearchCV` dengan `RandomizedSearchCV`.**\n",
        "\n",
        "`RandomizedSearchCV` bekerja dengan cara yang sangat mirip dengan `GridSearchCV`, tetapi ia tidak mencoba semua kombinasi yang mungkin. Sebaliknya, ia mengevaluasi sejumlah kombinasi acak yang telah ditentukan dengan memilih nilai acak untuk setiap *hyperparameter* pada setiap iterasi. Ini lebih efisien ketika ruang pencarian *hyperparameter* besar 125].\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import expon, reciprocal\n",
        "\n",
        "# Contoh distribusi untuk RandomizedSearchCV\n",
        "param_distribs = {\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'C': reciprocal(20, 200000),\n",
        "        'gamma': expon(scale=1.0),\n",
        "    }\n",
        "\n",
        "svm_reg = SVR()\n",
        "rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n",
        "                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n",
        "                                verbose=2, random_state=42)\n",
        "# rnd_search.fit(housing_prepared, housing_labels)\n",
        "```\n",
        "Hasil dari `RandomizedSearchCV` seringkali sebanding dengan `GridSearchCV` tetapi dalam waktu yang lebih singkat.\n",
        "\n",
        "**3. Coba tambahkan transformer di dalam pipeline persiapan untuk memilih hanya atribut yang paling penting.**\n",
        "\n",
        "Kita dapat membuat transformer kustom yang memilih *k* fitur teratas berdasarkan skor *feature importance* yang diberikan oleh model seperti `RandomForestRegressor`.\n",
        "\n",
        "```python\n",
        "# Transformer kustom untuk memilih fitur teratas\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "def indices_of_top_k(arr, k):\n",
        "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
        "\n",
        "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, feature_importances, k):\n",
        "        self.feature_importances = feature_importances\n",
        "        self.k = k\n",
        "    def fit(self, X, y=None):\n",
        "        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X[:, self.feature_indices_]\n",
        "```\n",
        "Transformer ini kemudian dapat dimasukkan ke dalam pipeline `full_pipeline` untuk secara otomatis memilih fitur-fitur yang paling relevan.\n",
        "\n",
        "**4. Coba buat satu pipeline tunggal yang melakukan persiapan data penuh ditambah prediksi akhir.**\n",
        "\n",
        "Kita dapat menggabungkan pipeline persiapan (`full_pipeline`) dengan model prediktor akhir (misalnya, `RandomForestRegressor`) menjadi satu pipeline besar.\n",
        "\n",
        "```python\n",
        "# Gabungkan pipeline persiapan dengan model SVR\n",
        "prepare_select_and_predict_pipeline = Pipeline([\n",
        "    ('preparation', full_pipeline),\n",
        "    # ('feature_selection', TopFeatureSelector(...)), # Bisa ditambahkan\n",
        "    ('svm_reg', SVR(**rnd_search.best_params_))\n",
        "])\n",
        "\n",
        "# prepare_select_and_predict_pipeline.fit(housing, housing_labels)\n",
        "```\n",
        "Pipeline tunggal ini sangat praktis karena menyederhanakan proses. Kita bisa melakukan `grid search` pada seluruh pipeline ini, bahkan mencoba berbagai opsi persiapan data secara otomatis.\n",
        "\n",
        "**5. Jelajahi beberapa opsi persiapan secara otomatis menggunakan `GridSearchCV`.**\n",
        "\n",
        "Dengan pipeline tunggal dari latihan sebelumnya, kita dapat menggunakan `GridSearchCV` untuk secara otomatis menemukan apakah akan menambahkan atau tidak fitur-fitur kombinasi (`attribs_adder`) atau apakah akan menggunakan pemilih fitur (`feature_selection`).\n",
        "\n",
        "```python\n",
        "param_grid = [{\n",
        "    'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
        "    # 'feature_selection__k': list(range(1, len(feature_importances) + 1)) # Contoh\n",
        "}]\n",
        "\n",
        "# grid_search = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n",
        "#                            scoring='neg_mean_squared_error', verbose=2)\n",
        "# grid_search.fit(housing, housing_labels)\n",
        "```\n",
        "Pendekatan ini mengotomatiskan sebagian besar proses *fine-tuning* dan eksplorasi, menjadikannya sangat kuat untuk menemukan solusi terbaik.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "S9_hZ2I127Oo"
      }
    }
  ]
}